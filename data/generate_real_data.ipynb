{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def generate_sp500_data():\n",
    "    print(\"--- Starting S&P 500 Data Download ---\")\n",
    "    \n",
    "    # 1. Get S&P 500 tickers from Wikipedia\n",
    "    try:\n",
    "        payload = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "        sp500_table = payload[0]\n",
    "        tickers = sp500_table['Symbol'].values.tolist()\n",
    "        # Fix tickers with dots (e.g., BRK.B -> BRK-B) for Yahoo Finance\n",
    "        tickers = [ticker.replace('.', '-') for ticker in tickers]\n",
    "        print(f\"Found {len(tickers)} tickers on Wikipedia.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Wikipedia list: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Select top 200 tickers (or all if you prefer)\n",
    "    # Using the first 200 ensures we get major companies often listed first or by weight if the table is sorted\n",
    "    selected_tickers = tickers[:200] \n",
    "\n",
    "    # 3. Define date range: Last 20 years\n",
    "    end_date = datetime.datetime.now()\n",
    "    start_date = end_date - datetime.timedelta(days=20*365)\n",
    "    \n",
    "    print(f\"Downloading data for {len(selected_tickers)} companies from {start_date.date()} to {end_date.date()}...\")\n",
    "    print(\"This may take a minute...\")\n",
    "\n",
    "    # 4. Download data (Adj Close only)\n",
    "    # threads=True uses multi-threading to download faster\n",
    "    data = yf.download(selected_tickers, start=start_date, end=end_date, progress=True, threads=True)['Adj Close']\n",
    "    \n",
    "    # 5. Clean Data\n",
    "    # Remove columns with too many NaNs (companies that didn't exist 20 years ago)\n",
    "    # We require at least 95% of the data points to be present\n",
    "    threshold = 0.95 * len(data)\n",
    "    before_count = data.shape[1]\n",
    "    data_clean = data.dropna(axis=1, thresh=threshold)\n",
    "    \n",
    "    # Forward fill remaining small gaps and drop any leading NaNs\n",
    "    data_clean = data_clean.ffill().dropna()\n",
    "    \n",
    "    after_count = data_clean.shape[1]\n",
    "    print(f\"Data cleaning: kept {after_count} of {before_count} companies that have ~20 years of history.\")\n",
    "    print(f\"Final dataset shape: {data_clean.shape}\")\n",
    "\n",
    "    # 6. Save to CSV\n",
    "    output_path = 'data/portfolio_allocation.csv'\n",
    "    \n",
    "    # Ensure data directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    data_clean.to_csv(output_path)\n",
    "    print(f\"Success! File saved to: {output_path}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(data_clean.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure yfinance is installed: pip install yfinance\n",
    "    generate_sp500_data()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
